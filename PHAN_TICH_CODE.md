# üìä Ph√¢n T√≠ch Chi Ti·∫øt Code trong model_train_v3_kaggle.py

## T·ªïng quan: 1321 d√≤ng code

---

## ‚úÖ PH·∫¶N CODE ƒê∆Ø·ª¢C D√ôNG TRONG TRAINING (C·∫¶N GI·ªÆ L·∫†I)

### 1. **Imports & Setup** (d√≤ng 11-69) - ‚úÖ C·∫¶N
- Standard imports (argparse, os, Path, etc.)
- Mixed precision setup (quan tr·ªçng cho EfficientNet)
- CRF import (optional, nh∆∞ng kh√¥ng ·∫£nh h∆∞·ªüng n·∫øu kh√¥ng c√≥)
- EfficientNet imports
- **S·ªë d√≤ng:** ~59 d√≤ng
- **T√¨nh tr·∫°ng:** C·∫¶N THI·∫æT

### 2. **Data Loading Functions** (d√≤ng 81-135) - ‚úÖ C·∫¶N
- `read_labelmap()` - ƒê·ªçc labelmap file
- `build_color_to_index()` - Chuy·ªÉn m√†u ‚Üí index
- `mask_rgb_to_index()` - Convert RGB mask ‚Üí class index
- `set_seed()` - Set random seed
- **S·ªë d√≤ng:** ~55 d√≤ng
- **T√¨nh tr·∫°ng:** C·∫¶N THI·∫æT cho training

### 3. **Metrics** (d√≤ng 142-158) - ‚úÖ C·∫¶N
- `compute_confusion_matrix()` - T√≠nh confusion matrix
- `miou_from_confmat()` - T√≠nh mIoU t·ª´ confusion matrix
- **S·ªë d√≤ng:** ~17 d√≤ng
- **T√¨nh tr·∫°ng:** C·∫¶N THI·∫æT cho evaluation

### 4. **Advanced Data Augmentation** (d√≤ng 160-253) - ‚ö†Ô∏è T√ôY CH·ªåN
- `AdvancedAugmentation` class:
  - `_elastic_transform()` - Elastic deformation
  - `_color_jitter()` - Brightness, contrast, saturation
  - `_random_rotation()` - Random rotation
  - `apply()` - Apply all augmentations
- **S·ªë d√≤ng:** ~94 d√≤ng
- **T√¨nh tr·∫°ng:** CH·ªà D√ôNG KHI `--use_advanced_aug=True`
- **Khuy·∫øn ngh·ªã:** Gi·ªØ l·∫°i v√¨ c√≥ th·ªÉ b·∫≠t/t·∫Øt d·ªÖ d√†ng

### 5. **Class Imbalance Handling** (d√≤ng 255-271) - ‚ö†Ô∏è T√ôY CH·ªåN
- `compute_class_weights()` - T√≠nh class weights t·ª´ masks
- **S·ªë d√≤ng:** ~17 d√≤ng
- **T√¨nh tr·∫°ng:** CH·ªà D√ôNG KHI `--loss=weighted_ce`
- **Khuy·∫øn ngh·ªã:** Gi·ªØ l·∫°i v√¨ h·ªØu √≠ch khi c√≥ class imbalance

### 6. **Loss Functions** (d√≤ng 273-369) - ‚ö†Ô∏è T√ôY CH·ªåN
- `sparse_ce_ignore_index()` - Standard CE loss ‚úÖ D√ôNG
- `weighted_sparse_ce_ignore_index()` - Weighted CE ‚ö†Ô∏è CH·ªà D√ôNG KHI `--loss=weighted_ce`
- `focal_loss()` - Focal Loss ‚ö†Ô∏è CH·ªà D√ôNG KHI `--loss=focal`
- `tversky_loss()` - Tversky Loss ‚ö†Ô∏è CH·ªà D√ôNG KHI `--loss=tversky`
- **S·ªë d√≤ng:** ~97 d√≤ng
- **T√¨nh tr·∫°ng:** 
  - `sparse_ce_ignore_index`: LU√îN D√ôNG (default)
  - C√°c loss kh√°c: CH·ªà D√ôNG KHI ch·ªçn t∆∞∆°ng ·ª©ng
- **Khuy·∫øn ngh·ªã:** Gi·ªØ l·∫°i v√¨ c√≥ th·ªÉ ch·ªçn loss function khi train

### 7. **Model Architectures** (d√≤ng 371-547) - ‚ö†Ô∏è T√ôY CH·ªåN
- `double_conv_block()` - Building block ‚úÖ D√ôNG
- `attention_gate()` - Attention mechanism ‚ö†Ô∏è CH·ªà D√ôNG V·ªöI `attention_unet`
- `downsample_block()` - Encoder block ‚úÖ D√ôNG
- `upsample_block()` - Decoder block ‚úÖ D√ôNG
- `build_attention_unet()` - Attention U-Net ‚ö†Ô∏è CH·ªà D√ôNG KHI `--architecture=attention_unet`
- `build_unet_with_backbone()` - U-Net + EfficientNet ‚ö†Ô∏è CH·ªà D√ôNG KHI `--architecture=unet_backbone`
- `build_unet_with_boundary()` - Standard U-Net ‚úÖ D√ôNG (default)
- **S·ªë d√≤ng:** ~177 d√≤ng
- **T√¨nh tr·∫°ng:** M·ªói architecture ch·ªâ d√πng khi ƒë∆∞·ª£c ch·ªçn
- **Khuy·∫øn ngh·ªã:** Gi·ªØ l·∫°i v√¨ c√≥ th·ªÉ ch·ªçn architecture kh√°c nhau

### 8. **Boundary Targets** (d√≤ng 549-556) - ‚úÖ C·∫¶N
- `make_boundary_targets()` - T·∫°o boundary targets t·ª´ mask
- **S·ªë d√≤ng:** ~8 d√≤ng
- **T√¨nh tr·∫°ng:** C·∫¶N THI·∫æT (model c√≥ boundary head)
- **Khuy·∫øn ngh·ªã:** GI·ªÆ L·∫†I

### 9. **Dataset Class** (d√≤ng 606-714) - ‚úÖ C·∫¶N
- `EnhancedMultiRootVOCDataset` class:
  - `__init__()` - Kh·ªüi t·∫°o dataset
  - `_load_sample()` - Load image v√† mask
  - `_random_resize()` - Random resize augmentation
  - `_random_crop()` - Random crop
  - `_hflip()` - Horizontal flip
  - `_center_crop_or_resize()` - Validation preprocessing
  - `get_item()` - Get m·ªôt sample
- **S·ªë d√≤ng:** ~109 d√≤ng
- **T√¨nh tr·∫°ng:** C·∫¶N THI·∫æT cho training
- **Khuy·∫øn ngh·ªã:** GI·ªÆ L·∫†I

### 10. **TF Data Pipeline** (d√≤ng 716-738) - ‚úÖ C·∫¶N
- `make_tf_dataset()` - T·∫°o tf.data.Dataset
- **S·ªë d√≤ng:** ~23 d√≤ng
- **T√¨nh tr·∫°ng:** C·∫¶N THI·∫æT cho training
- **Khuy·∫øn ngh·ªã:** GI·ªÆ L·∫†I

### 11. **Evaluation Callback** (d√≤ng 740-803) - ‚úÖ C·∫¶N
- `EvalCallback` class - Callback ƒë·ªÉ evaluate model m·ªói epoch
- **S·ªë d√≤ng:** ~64 d√≤ng
- **T√¨nh tr·∫°ng:** C·∫¶N THI·∫æT cho training
- **Khuy·∫øn ngh·ªã:** GI·ªÆ L·∫†I

### 12. **Main Training Function** (d√≤ng 1033-1319) - ‚úÖ C·∫¶N
- `main_unet()` - H√†m ch√≠nh ƒë·ªÉ train model
- **S·ªë d√≤ng:** ~287 d√≤ng
- **T√¨nh tr·∫°ng:** C·∫¶N THI·∫æT
- **Khuy·∫øn ngh·ªã:** GI·ªÆ L·∫†I

---

## ‚ùå PH·∫¶N CODE KH√îNG D√ôNG TRONG TRAINING (C√ì TH·ªÇ X√ìA)

### 1. **Instance Segmentation Function** (d√≤ng 558-604) - ‚ùå KH√îNG D√ôNG
- `instances_from_sem_and_boundary()` - T·∫°o instance map t·ª´ semantic + boundary
- **S·ªë d√≤ng:** ~47 d√≤ng
- **T√¨nh tr·∫°ng:** KH√îNG ƒê∆Ø·ª¢C G·ªåI trong `main_unet()` ho·∫∑c training flow
- **D√πng ·ªü ƒë√¢u:** Ch·ªâ ƒë∆∞·ª£c d√πng trong c√°c file kh√°c (model_train_Dao_code.py, model_train_Hai_code.py)
- **Khuy·∫øn ngh·ªã:** ‚ùå X√ìA N·∫æU KH√îNG D√ôNG INSTANCE SEGMENTATION
- **Imports li√™n quan c·∫ßn x√≥a:**
  - `from skimage.feature import peak_local_max` (d√≤ng 48)
  - `from skimage.segmentation import watershed` (d√≤ng 49)

### 2. **Post-Processing Pipeline** (d√≤ng 805-887) - ‚ùå KH√îNG D√ôNG TRONG TRAINING
- `PostProcessor` class:
  - `apply_morphology()` - Morphological operations
  - `filter_small_blobs()` - Filter small connected components
  - `apply_crf()` - CRF refinement
  - `process()` - Full pipeline
- **S·ªë d√≤ng:** ~83 d√≤ng
- **T√¨nh tr·∫°ng:** CH·ªà D√ôNG TRONG `inference_pipeline()`, KH√îNG D√ôNG TRONG TRAINING
- **Khuy·∫øn ngh·ªã:** ‚ùå X√ìA N·∫æU CH·ªà D√ôNG CHO TRAINING
- **Imports li√™n quan c·∫ßn x√≥a:**
  - `from skimage.morphology import binary_opening, binary_closing, disk` (d√≤ng 50)
  - `from skimage.measure import label, regionprops` (d√≤ng 51)
  - `pydensecrf` import (d√≤ng 52-61)

### 3. **Test Time Augmentation** (d√≤ng 889-953) - ‚ùå KH√îNG D√ôNG TRONG TRAINING
- `TTAInference` class:
  - `_apply_transform()` - Apply transformations
  - `_reverse_transform()` - Reverse transformations
  - `predict()` - Predict with TTA
- **S·ªë d√≤ng:** ~65 d√≤ng
- **T√¨nh tr·∫°ng:** CH·ªà D√ôNG TRONG `inference_pipeline()`, KH√îNG D√ôNG TRONG TRAINING
- **Khuy·∫øn ngh·ªã:** ‚ùå X√ìA N·∫æU CH·ªà D√ôNG CHO TRAINING

### 4. **Inference Pipeline** (d√≤ng 955-1031) - ‚ùå KH√îNG D√ôNG TRONG TRAINING
- `inference_pipeline()` - Complete inference function v·ªõi TTA v√† post-processing
- **S·ªë d√≤ng:** ~77 d√≤ng
- **T√¨nh tr·∫°ng:** CH·ªà D√ôNG TRONG FILE RI√äNG (`inference_improved.py`), KH√îNG D√ôNG TRONG TRAINING
- **Khuy·∫øn ngh·ªã:** ‚ùå X√ìA N·∫æU CH·ªà D√ôNG CHO TRAINING
- **Dependencies:** Ph·ª• thu·ªôc v√†o `TTAInference` v√† `PostProcessor`

---

## üìä T√ìM T·∫ÆT THEO TR·∫†NG TH√ÅI S·ª¨ D·ª§NG

### ‚úÖ **LU√îN D√ôNG (Core Training Code):**
- Data loading functions (d√≤ng 81-135)
- Metrics (d√≤ng 142-158)
- Basic loss: `sparse_ce_ignore_index()` (d√≤ng 274-288)
- Model building blocks (d√≤ng 372-407)
- Standard U-Net: `build_unet_with_boundary()` (d√≤ng 532-547)
- Boundary targets: `make_boundary_targets()` (d√≤ng 549-556)
- Dataset class (d√≤ng 606-714)
- TF Data Pipeline (d√≤ng 716-738)
- Evaluation Callback (d√≤ng 740-803)
- Main function (d√≤ng 1033-1319)
- **T·ªïng:** ~800-900 d√≤ng (Core code)

### ‚ö†Ô∏è **T√ôY CH·ªåN (C√≥ th·ªÉ b·∫≠t/t·∫Øt):**
- Advanced Augmentation (d√≤ng 160-253) - `--use_advanced_aug`
- Class weights (d√≤ng 255-271) - `--loss=weighted_ce`
- Weighted CE loss (d√≤ng 290-309) - `--loss=weighted_ce`
- Focal Loss (d√≤ng 311-338) - `--loss=focal`
- Tversky Loss (d√≤ng 340-369) - `--loss=tversky`
- Attention U-Net (d√≤ng 418-441) - `--architecture=attention_unet`
- EfficientNet Backbone (d√≤ng 443-530) - `--architecture=unet_backbone`
- **T·ªïng:** ~300-400 d√≤ng (Optional features)

### ‚ùå **KH√îNG D√ôNG TRONG TRAINING (Ch·ªâ d√πng khi inference):**
- Instance Segmentation (d√≤ng 558-604) - ~47 d√≤ng
- Post-Processing (d√≤ng 805-887) - ~83 d√≤ng
- TTA (d√≤ng 889-953) - ~65 d√≤ng
- Inference Pipeline (d√≤ng 955-1031) - ~77 d√≤ng
- **T·ªïng:** ~272 d√≤ng (Inference-only code)

---

## üí° KHUY·∫æN NGH·ªä X√ìA

### N·∫øu b·∫°n CH·ªà D√ôNG CHO TRAINING v√† c√≥ file inference ri√™ng:

**ƒê√£ x√≥a (~290 d√≤ng):**
1. ‚úÖ `instances_from_sem_and_boundary()` - ~47 d√≤ng
2. ‚úÖ `PostProcessor` class - ~83 d√≤ng
3. ‚úÖ `TTAInference` class - ~65 d√≤ng
4. ‚úÖ `inference_pipeline()` - ~77 d√≤ng
5. ‚úÖ Imports kh√¥ng c·∫ßn thi·∫øt (peak_local_max, watershed, pydensecrf, morphology, measure) - ~18 d√≤ng

**Imports c√≥ th·ªÉ x√≥a:**
- `from skimage.feature import peak_local_max` (d√≤ng 48)
- `from skimage.segmentation import watershed` (d√≤ng 49)
- `from skimage.morphology import binary_opening, binary_closing, disk` (d√≤ng 50)
- `from skimage.measure import label, regionprops` (d√≤ng 51)
- `pydensecrf` import block (d√≤ng 52-61)

**T·ªïng c√≥ th·ªÉ gi·∫£m:** ~272 d√≤ng code + ~10 d√≤ng imports = **~282 d√≤ng**

### N·∫øu b·∫°n mu·ªën GI·ªÆ T·∫§T C·∫¢ T√çNH NƒÇNG:
- Gi·ªØ l·∫°i t·∫•t c·∫£ v√¨ c√≥ th·ªÉ d√πng sau n√†y
- Code hi·ªán t·∫°i ƒë√£ ƒë∆∞·ª£c t·ªï ch·ª©c t·ªët, kh√¥ng ·∫£nh h∆∞·ªüng performance

---

## üìà K·∫æT QU·∫¢ SAU KHI X√ìA

**Tr∆∞·ªõc:** 1321 d√≤ng
**Sau khi x√≥a inference-only code:** ~1040 d√≤ng
**Gi·∫£m:** ~282 d√≤ng (21%)

**Code c√≤n l·∫°i s·∫Ω bao g·ªìm:**
- ‚úÖ Core training code (~800-900 d√≤ng)
- ‚úÖ Optional features (~300-400 d√≤ng - c√≥ th·ªÉ b·∫≠t/t·∫Øt)
- ‚ùå ƒê√£ x√≥a inference-only code (~272 d√≤ng)

---

## üéØ QUY·∫æT ƒê·ªäNH

B·∫°n mu·ªën:
1. **X√≥a inference-only code** ‚Üí Code g·ªçn h∆°n, ch·ªâ t·∫≠p trung v√†o training
2. **Gi·ªØ l·∫°i t·∫•t c·∫£** ‚Üí C√≥ th·ªÉ d√πng inference pipeline sau n√†y

Cho t√¥i bi·∫øt b·∫°n mu·ªën l√†m g√¨ nh√©!

