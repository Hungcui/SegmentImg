# üìö H∆∞·ªõng D·∫´n Chi Ti·∫øt Train Model tr√™n Kaggle GPU

## üéØ T·ªïng Quan

H∆∞·ªõng d·∫´n n√†y s·∫Ω gi√∫p b·∫°n train model segmentation tr√™n Kaggle v·ªõi GPU (P100, T4, ho·∫∑c T4 x2) m·ªôt c√°ch chi ti·∫øt t·ª´ng b∆∞·ªõc.

---

## üìã B∆∞·ªõc 1: Chu·∫©n B·ªã Dataset tr√™n Kaggle

### ‚úÖ C√°ch 1: T·∫°o Kaggle Dataset v·ªõi File ZIP (Khuy·∫øn Ngh·ªã - Nhanh Nh·∫•t)

**Tr√™n Kaggle:**

1. **Chu·∫©n b·ªã file ZIP tr√™n m√°y local:**
   - Zip to√†n b·ªô folder `data/` (ch·ª©a c√°c folder: cheetah, lion, wolf, tiger, hyena, fox)
   - File zip n√™n c√≥ c·∫•u tr√∫c: `data.zip` ‚Üí `data/cheetah/`, `data/lion/`, ...
   - Ho·∫∑c zip to√†n b·ªô, ƒë·∫£m b·∫£o khi gi·∫£i n√©n s·∫Ω c√≥ folder `data/` ·ªü root

2. **Truy c·∫≠p:** https://www.kaggle.com/datasets
3. **T·∫°o dataset m·ªõi:** Click **"New Dataset"**
4. **Upload file ZIP:**
   - K√©o th·∫£ file `data.zip` (ho·∫∑c t√™n file zip c·ªßa b·∫°n)
   - Upload `labelmap.txt` v√†o c√πng dataset
   - **L∆∞u √Ω:** C√≥ th·ªÉ upload nhi·ªÅu file c√πng l√∫c
5. **ƒê·∫∑t t√™n dataset:** V√≠ d·ª•: `animal-segmentation-dataset`
6. **Public ho·∫∑c Private:** Ch·ªçn theo nhu c·∫ßu
7. **Click "Create"**

**∆Øu ƒëi·ªÉm:**
- ‚úÖ Upload nhanh h∆°n (1 file thay v√¨ nhi·ªÅu folder)
- ‚úÖ Gi·ªØ nguy√™n c·∫•u tr√∫c th∆∞ m·ª•c
- ‚úÖ D·ªÖ qu·∫£n l√Ω v√† chia s·∫ª

### C√°ch 2: Upload T·ª´ng Folder (N·∫øu kh√¥ng zip)

**Tr√™n Kaggle:**

1. **Truy c·∫≠p:** https://www.kaggle.com/datasets
2. **T·∫°o dataset m·ªõi:** Click **"New Dataset"**
3. **Upload data:**
   - K√©o th·∫£ ho·∫∑c ch·ªçn c√°c folder ch·ª©a data:
     - `data/cheetah/`
     - `data/lion/`
     - `data/wolf/`
     - `data/tiger/`
     - `data/hyena/`
     - `data/fox/`
   - Upload `labelmap.txt` v√†o root c·ªßa dataset
4. **ƒê·∫∑t t√™n dataset:** V√≠ d·ª•: `animal-segmentation-dataset`
5. **Public ho·∫∑c Private:** Ch·ªçn theo nhu c·∫ßu
6. **Click "Create"**

### C√°ch 3: Upload Tr·ª±c Ti·∫øp v√†o Notebook (Cho file nh·ªè)

- S·ª≠ d·ª•ng file browser trong Kaggle notebook ƒë·ªÉ upload tr·ª±c ti·∫øp
- File s·∫Ω ƒë∆∞·ª£c l∆∞u v√†o `/kaggle/working/`

---

## üöÄ B∆∞·ªõc 2: T·∫°o Notebook tr√™n Kaggle

1. **Truy c·∫≠p:** https://www.kaggle.com/code
2. **T·∫°o notebook m·ªõi:** Click **"New Notebook"**
3. **Ch·ªçn GPU:**
   - Settings ‚Üí Accelerator ‚Üí **GPU** (P100, T4, ho·∫∑c 2xT4)
   - **L∆∞u √Ω:** Kaggle cho ph√©p GPU mi·ªÖn ph√≠ nh∆∞ng c√≥ gi·ªõi h·∫°n th·ªùi gian
4. **Add Dataset:**
   - Click **"Add Data"** b√™n ph·∫£i
   - T√¨m v√† ch·ªçn dataset b·∫°n ƒë√£ t·∫°o ·ªü B∆∞·ªõc 1
   - Dataset s·∫Ω ƒë∆∞·ª£c mount v√†o `/kaggle/input/YOUR_DATASET_NAME/`

---

## üéØ So S√°nh GPU: 2xT4 vs P100 - N√™n Ch·ªçn G√¨?

### üìä B·∫£ng So S√°nh Chi Ti·∫øt

| Ti√™u ch√≠ | **P100 (Single)** | **2xT4 (Dual)** | **Khuy·∫øn ngh·ªã** |
|----------|-------------------|-----------------|-----------------|
| **VRAM t·ªïng** | 16GB | 32GB (16GB x2) | ‚≠ê 2xT4 cho model l·ªõn |
| **VRAM per GPU** | 16GB | 16GB | = |
| **Compute Power** | Cao h∆°n T4 | Trung b√¨nh | ‚≠ê P100 cho t·ªëc ƒë·ªô |
| **Batch Size l·ªõn nh·∫•t** | 12-16 (512x512) | 20-32 (512x512) | ‚≠ê 2xT4 cho batch l·ªõn |
| **Patch Size l·ªõn nh·∫•t** | 512-640 | 768-1024 | ‚≠ê 2xT4 cho resolution cao |
| **Multi-GPU Setup** | ‚ùå Kh√¥ng c·∫ßn | ‚úÖ C·∫ßn config | P100 ƒë∆°n gi·∫£n h∆°n |
| **Training Speed** | ‚ö° Nhanh h∆°n | üê¢ Ch·∫≠m h∆°n (overhead) | ‚≠ê P100 nhanh h∆°n |
| **ƒê·ªô ph·ª©c t·∫°p code** | ‚úÖ ƒê∆°n gi·∫£n | ‚ö†Ô∏è C·∫ßn multi-GPU strategy | ‚≠ê P100 d·ªÖ h∆°n |
| **Model l·ªõn (EfficientNetB4+)** | ‚ö†Ô∏è C√≥ th·ªÉ OOM | ‚úÖ ƒê·ªß VRAM | ‚≠ê 2xT4 cho model l·ªõn |
| **Kaggle Availability** | Th∆∞·ªùng c√≥ | √çt h∆°n | P100 d·ªÖ ki·∫øm h∆°n |

### üéØ Khuy·∫øn Ngh·ªã Ch·ªçn GPU

#### ‚úÖ **Ch·ªçn P100 khi:**
- ‚úÖ Model nh·ªè-trung b√¨nh (EfficientNetB0-B3)
- ‚úÖ Mu·ªën training nhanh
- ‚úÖ Kh√¥ng mu·ªën ph·ª©c t·∫°p code (single GPU)
- ‚úÖ Patch size ‚â§ 640x640
- ‚úÖ Batch size ‚â§ 16 l√† ƒë·ªß
- ‚úÖ **ƒê√¢y l√† l·ª±a ch·ªçn t·ªët nh·∫•t cho EfficientNetB3!**

#### ‚úÖ **Ch·ªçn 2xT4 khi:**
- ‚úÖ Model l·ªõn (EfficientNetB4-B7, ResNet101+)
- ‚úÖ C·∫ßn patch size l·ªõn (768x768+)
- ‚úÖ C·∫ßn batch size l·ªõn (>20)
- ‚úÖ S·∫µn s√†ng config multi-GPU
- ‚úÖ Dataset r·∫•t l·ªõn, c·∫ßn throughput cao

### üí° K·∫øt Lu·∫≠n cho EfficientNetB3

**Khuy·∫øn ngh·ªã: P100** ‚úÖ

**L√Ω do:**
1. ‚úÖ EfficientNetB3 v·ª´a ph·∫£i, P100 ƒë·ªß VRAM
2. ‚úÖ Training nhanh h∆°n (kh√¥ng c√≥ overhead multi-GPU)
3. ‚úÖ Code ƒë∆°n gi·∫£n h∆°n (single GPU)
4. ‚úÖ Patch size 512x512 l√† t·ªëi ∆∞u, P100 handle t·ªët
5. ‚úÖ Batch size 8-12 ƒë·ªß cho training ·ªïn ƒë·ªãnh

**Ch·ªâ ch·ªçn 2xT4 n·∫øu:**
- B·∫°n mu·ªën train EfficientNetB4 tr·ªü l√™n
- C·∫ßn patch size ‚â• 768x768
- C·∫ßn batch size ‚â• 20

---

### üöÄ Multi-GPU Training v·ªõi 2xT4 (N·∫øu C·∫ßn)

N·∫øu b·∫°n ch·ªçn 2xT4, c·∫ßn setup MirroredStrategy ƒë·ªÉ s·ª≠ d·ª•ng c·∫£ 2 GPU:

```python
# Setup Multi-GPU Strategy (ch·ªâ c·∫ßn n·∫øu c√≥ 2xT4)
import tensorflow as tf

gpus = tf.config.list_physical_devices('GPU')
if len(gpus) > 1:
    # T·∫°o MirroredStrategy ƒë·ªÉ s·ª≠ d·ª•ng t·∫•t c·∫£ GPU
    strategy = tf.distribute.MirroredStrategy()
    print(f"‚úÖ Using {strategy.num_replicas_in_sync} GPU(s)")
    
    # Build model v√† train trong strategy scope
    with strategy.scope():
        # Build model ·ªü ƒë√¢y
        model = build_unet_with_backbone(
            num_classes=num_classes,
            backbone="efficientnet",
            backbone_name="EfficientNetB3"
        )
        # Compile model
        model.compile(...)
        
    # Training s·∫Ω t·ª± ƒë·ªông distribute qua c√°c GPU
    model.fit(train_ds, epochs=EPOCHS, ...)
else:
    # Single GPU - kh√¥ng c·∫ßn strategy
    model = build_unet_with_backbone(...)
    model.compile(...)
    model.fit(train_ds, epochs=EPOCHS, ...)
```

**L∆∞u √Ω Multi-GPU:**
- Batch size s·∫Ω ƒë∆∞·ª£c chia ƒë·ªÅu cho c√°c GPU (batch_size=16 ‚Üí 8 per GPU)
- Effective batch size = batch_size √ó s·ªë_GPU
- Overhead communication c√≥ th·ªÉ l√†m ch·∫≠m 10-20%
- Ch·ªâ n√™n d√πng khi single GPU kh√¥ng ƒë·ªß VRAM

---

## üîß B∆∞·ªõc 3: Setup M√¥i Tr∆∞·ªùng

### Cell 1: Ki·ªÉm tra GPU

```python
# Ki·ªÉm tra GPU c√≥ s·∫µn kh√¥ng
import tensorflow as tf
print("TensorFlow version:", tf.__version__)
gpus = tf.config.list_physical_devices('GPU')
print(f"GPU Available: {len(gpus)} GPU(s)")
print(f"GPU Details: {gpus}")

# Ki·ªÉm tra GPU details
if gpus:
    for i, gpu in enumerate(gpus):
        print(f"\nGPU {i}: {gpu}")
        print(f"  Name: {gpu.name}")
        # Enable memory growth ƒë·ªÉ tr√°nh allocate to√†n b·ªô VRAM
        try:
            tf.config.experimental.set_memory_growth(gpu, True)
            print(f"  ‚úÖ Memory growth enabled")
        except RuntimeError as e:
            print(f"  ‚ö†Ô∏è Cannot set memory growth: {e}")
    
    # N·∫øu c√≥ nhi·ªÅu GPU, c√≥ th·ªÉ d√πng multi-GPU strategy
    if len(gpus) > 1:
        print(f"\nüöÄ Multi-GPU detected: {len(gpus)} GPUs")
        print("üí° Tip: ƒê·ªÉ s·ª≠ d·ª•ng multi-GPU, c·∫ßn setup MirroredStrategy (xem ph·∫ßn Multi-GPU Training)")
    else:
        print(f"\n‚úÖ Single GPU setup - ƒê∆°n gi·∫£n v√† hi·ªáu qu·∫£!")
else:
    print("‚ö†Ô∏è Kh√¥ng c√≥ GPU! Vui l√≤ng ch·ªçn GPU trong Settings ‚Üí Accelerator")
```

### Cell 2: C√†i ƒë·∫∑t Dependencies

```python
# C√†i ƒë·∫∑t c√°c package c·∫ßn thi·∫øt
!pip install -q tensorflow>=2.13.0
!pip install -q keras>=2.13.0
!pip install -q scikit-image
!pip install -q scipy
!pip install -q opencv-python
!pip install -q pillow

# Optional: CRF post-processing (n·∫øu c·∫ßn)
# !pip install -q git+https://github.com/lucasb-eyer/pydensecrf.git

print("‚úÖ ƒê√£ c√†i ƒë·∫∑t dependencies!")
```

### Cell 3: Gi·∫£i N√©n File ZIP Data (N·∫øu Dataset l√† File ZIP)

```python
import os
import zipfile
from pathlib import Path

# T√™n dataset c·ªßa b·∫°n (thay YOUR_DATASET_NAME b·∫±ng t√™n th·ª±c t·∫ø)
DATASET_NAME = "YOUR_DATASET_NAME"  # V√≠ d·ª•: "animal-segmentation-dataset"
DATASET_PATH = f"/kaggle/input/{DATASET_NAME}"

# T√¨m file zip trong dataset
zip_files = list(Path(DATASET_PATH).glob("*.zip"))
if not zip_files:
    # Th·ª≠ c√°c t√™n dataset kh√°c
    possible_names = ["segmentimg", "animal-segmentation-dataset", "segmentation-data"]
    for name in possible_names:
        test_path = f"/kaggle/input/{name}"
        zip_files = list(Path(test_path).glob("*.zip"))
        if zip_files:
            DATASET_PATH = test_path
            DATASET_NAME = name
            break

if zip_files:
    zip_file = zip_files[0]  # L·∫•y file zip ƒë·∫ßu ti√™n
    print(f"üì¶ T√¨m th·∫•y file ZIP: {zip_file}")
    
    # Th∆∞ m·ª•c gi·∫£i n√©n (s·∫Ω gi·∫£i n√©n v√†o /kaggle/working/)
    EXTRACT_DIR = "/kaggle/working"
    
    # Ki·ªÉm tra xem ƒë√£ gi·∫£i n√©n ch∆∞a
    data_dir = Path(EXTRACT_DIR) / "data"
    if data_dir.exists() and any(data_dir.iterdir()):
        print("‚úÖ Data ƒë√£ ƒë∆∞·ª£c gi·∫£i n√©n tr∆∞·ªõc ƒë√≥, b·ªè qua...")
    else:
        print(f"üìÇ ƒêang gi·∫£i n√©n {zip_file.name} v√†o {EXTRACT_DIR}...")
        with zipfile.ZipFile(zip_file, 'r') as zip_ref:
            zip_ref.extractall(EXTRACT_DIR)
        print("‚úÖ Gi·∫£i n√©n ho√†n t·∫•t!")
        
        # Ki·ªÉm tra c·∫•u tr√∫c sau khi gi·∫£i n√©n
        if data_dir.exists():
            print(f"‚úÖ T√¨m th·∫•y folder data t·∫°i: {data_dir}")
            subfolders = [d.name for d in data_dir.iterdir() if d.is_dir()]
            print(f"üìÅ C√°c folder trong data: {subfolders}")
        else:
            # C√≥ th·ªÉ file zip kh√¥ng c√≥ folder data ·ªü root
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y folder 'data' sau khi gi·∫£i n√©n")
            print("üí° Ki·ªÉm tra c·∫•u tr√∫c file zip c·ªßa b·∫°n")
else:
    print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file ZIP trong dataset")
    print("üí° N·∫øu dataset ƒë√£ l√† folder (kh√¥ng ph·∫£i zip), b·ªè qua b∆∞·ªõc n√†y")
```

### Cell 4: Upload Code

**Option A: Upload file tr·ª±c ti·∫øp**
```python
# S·ª≠ d·ª•ng file browser b√™n tr√°i ƒë·ªÉ upload model_train_v3_kaggle.py
# File s·∫Ω ƒë∆∞·ª£c l∆∞u v√†o /kaggle/working/
```

**Option B: Copy code tr·ª±c ti·∫øp**
```python
# T·∫°o file m·ªõi
%%writefile /kaggle/working/model_train_v3_kaggle.py
# Paste to√†n b·ªô code t·ª´ file model_train_v3_kaggle.py v√†o ƒë√¢y
```

**Option C: N·∫øu code ƒë√£ ·ªü trong dataset**
```python
# Copy t·ª´ input dataset
!cp /kaggle/input/YOUR_DATASET_NAME/model_train_v3_kaggle.py /kaggle/working/
!cp /kaggle/input/YOUR_DATASET_NAME/labelmap.txt /kaggle/working/
```

---

## üìÅ B∆∞·ªõc 5: C·∫•u H√¨nh ƒê∆∞·ªùng D·∫´n

### Cell 5: Thi·∫øt l·∫≠p paths

```python
import os
from pathlib import Path

# T√¨m dataset trong /kaggle/input
# Thay YOUR_DATASET_NAME b·∫±ng t√™n dataset c·ªßa b·∫°n
DATASET_NAME = "YOUR_DATASET_NAME"  # V√≠ d·ª•: "animal-segmentation-dataset"
DATASET_PATH = f"/kaggle/input/{DATASET_NAME}"

# Ki·ªÉm tra dataset c√≥ t·ªìn t·∫°i kh√¥ng
if not Path(DATASET_PATH).exists():
    # Th·ª≠ c√°c t√™n kh√°c
    possible_names = ["segmentimg", "animal-segmentation-dataset", "segmentation-data"]
    for name in possible_names:
        test_path = f"/kaggle/input/{name}"
        if Path(test_path).exists():
            DATASET_PATH = test_path
            DATASET_NAME = name
            break

# X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n data (∆∞u ti√™n t·ª´ /kaggle/working/ n·∫øu ƒë√£ gi·∫£i n√©n)
WORKING_DATA = Path("/kaggle/working/data")
INPUT_DATA = Path(f"{DATASET_PATH}/data")

# Ki·ªÉm tra data ·ªü ƒë√¢u (ƒë√£ gi·∫£i n√©n hay ch∆∞a)
if WORKING_DATA.exists() and any(WORKING_DATA.iterdir()):
    # Data ƒë√£ ƒë∆∞·ª£c gi·∫£i n√©n v√†o /kaggle/working/data
    DATA_BASE = "/kaggle/working"
    print("‚úÖ S·ª≠ d·ª•ng data ƒë√£ gi·∫£i n√©n t·ª´ /kaggle/working/data")
elif INPUT_DATA.exists():
    # Data ·ªü trong dataset (ch∆∞a zip ho·∫∑c ƒë√£ gi·∫£i n√©n kh√°c)
    DATA_BASE = DATASET_PATH
    print(f"‚úÖ S·ª≠ d·ª•ng data t·ª´ dataset: {DATASET_PATH}")
else:
    # Fallback: t√¨m trong c√°c v·ªã tr√≠ kh√°c
    DATA_BASE = DATASET_PATH
    print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y folder data, ki·ªÉm tra l·∫°i c·∫•u tr√∫c dataset")

# C√°c dataset folders
DATA_ROOTS = [
    f"{DATA_BASE}/data/cheetah",
    f"{DATA_BASE}/data/lion",
    f"{DATA_BASE}/data/wolf",
    f"{DATA_BASE}/data/tiger",
    f"{DATA_BASE}/data/hyena",
    f"{DATA_BASE}/data/fox",
]

# Labelmap t·ª´ dataset ho·∫∑c working directory
LABELMAP_PATH = f"{DATASET_PATH}/labelmap.txt"
if not Path(LABELMAP_PATH).exists():
    LABELMAP_PATH = "/kaggle/working/labelmap.txt"

# Th∆∞ m·ª•c l∆∞u model (lu√¥n l∆∞u v√†o /kaggle/working/)
SAVE_DIR = "/kaggle/working/models"

# Ki·ªÉm tra paths
print("\n" + "="*60)
print("CHECKING DATA PATHS")
print("="*60)
print(f"Dataset: {DATASET_NAME}")
print(f"Data base: {DATA_BASE}")

all_exist = True
for root in DATA_ROOTS:
    root_path = Path(root)
    if root_path.exists():
        jpeg_path = root_path / "JPEGImages"
        n_images = len(list(jpeg_path.glob("*"))) if jpeg_path.exists() else 0
        print(f"‚úÖ {root_path.name}: {n_images} images")
    else:
        print(f"‚ùå {root_path.name} - KH√îNG T·ªíN T·∫†I!")
        all_exist = False

if Path(LABELMAP_PATH).exists():
    print(f"‚úÖ Labelmap: {LABELMAP_PATH}")
else:
    print(f"‚ùå Labelmap kh√¥ng t·ªìn t·∫°i: {LABELMAP_PATH}")
    print("üí° H√£y upload labelmap.txt v√†o dataset ho·∫∑c /kaggle/working/")
    all_exist = False

if not all_exist:
    print("\n‚ö†Ô∏è M·ªôt s·ªë paths kh√¥ng t·ªìn t·∫°i!")
    print("üí° Ki·ªÉm tra l·∫°i:")
    print("   1. ƒê√£ gi·∫£i n√©n file ZIP ch∆∞a? (Cell 3)")
    print("   2. T√™n dataset c√≥ ƒë√∫ng kh√¥ng?")
    print("   3. C·∫•u tr√∫c folder data c√≥ ƒë√∫ng kh√¥ng?")
else:
    print("\n‚úÖ T·∫•t c·∫£ paths ƒë·ªÅu h·ª£p l·ªá!")
print("="*60)
```

---

## üéì B∆∞·ªõc 6: Import Code v√† Train

### Cell 6: Import v√† Setup

```python
import sys

# CRITICAL: Set mixed precision policy to float32 BEFORE importing training code
# This prevents dtype conflicts when loading EfficientNet backbones
import tensorflow as tf
from keras import mixed_precision

try:
    mixed_precision.set_global_policy('float32')
    print("‚úÖ Mixed precision policy set to float32")
except:
    tf.keras.backend.set_floatx('float32')
    print("‚úÖ TensorFlow dtype set to float32")

# Disable mixed precision graph rewrite (for Kaggle/Colab environments)
try:
    tf.config.experimental.enable_mixed_precision_graph_rewrite(False)
    print("‚úÖ Mixed precision graph rewrite disabled")
except:
    pass

# Th√™m path ƒë·ªÉ import code
sys.path.insert(0, '/kaggle/working')  # Code ƒë√£ upload v√†o /kaggle/working

# Import code (ph·∫£i import SAU KHI set mixed precision policy)
from model_train_v3_kaggle import (
    read_labelmap, EnhancedMultiRootVOCDataset, 
    make_tf_dataset, build_attention_unet, build_unet_with_boundary,
    build_unet_plusplus, build_unet_with_backbone,
    sparse_ce_ignore_index, weighted_sparse_ce_ignore_index,
    focal_loss, tversky_loss, compute_class_weights,
    EvalCallback
)
import keras
import numpy as np
import random
from pathlib import Path

print("‚úÖ ƒê√£ import code!")
```

### Cell 7: C·∫•u h√¨nh Training

```python
# C·∫•u h√¨nh training
EPOCHS = 200  # Kaggle cho ph√©p train l√¢u h∆°n, c√≥ th·ªÉ tƒÉng epochs
BATCH_SIZE = 8  # P100/T4: 8-16, t√πy VRAM v√† crop_size
LR = 1e-3
CROP_SIZE = 512
ARCHITECTURE = "unet_backbone"  # 'unet', 'attention_unet', 'unet_plusplus', 'unet_backbone'
BACKBONE_NAME = "EfficientNetB3"  # 'EfficientNetB0', 'EfficientNetB3', 'EfficientNetB4'
LOSS = "focal"  # 'ce', 'weighted_ce', 'focal', 'tversky'
USE_ADVANCED_AUG = True
DEEP_SUPERVISION = False  # Ch·ªâ d√πng v·ªõi unet_plusplus

# CRITICAL: Kh√¥ng enable mixed precision ·ªü ƒë√¢y v√¨ ƒë√£ set float32 trong Cell 6
# Mixed precision s·∫Ω g√¢y conflict v·ªõi EfficientNet backbone

# Seed
random.seed(42)
np.random.seed(42)
tf.random.set_seed(42)

print(f"Config:")
print(f"  Architecture: {ARCHITECTURE}")
if ARCHITECTURE == "unet_backbone":
    print(f"  Backbone: {BACKBONE_NAME}")
print(f"  Loss: {LOSS}")
print(f"  Epochs: {EPOCHS}")
print(f"  Batch size: {BATCH_SIZE}")
print(f"  Crop size: {CROP_SIZE}")
```

### üìê Khuy·∫øn Ngh·ªã Patch Size cho EfficientNetB3 tr√™n GPU P100

**Cho EfficientNetB3 + U-Net decoder tr√™n GPU P100 (16GB VRAM):**

| Patch Size | Batch Size | ∆Øu ƒëi·ªÉm | Nh∆∞·ª£c ƒëi·ªÉm | Khuy·∫øn ngh·ªã |
|------------|------------|---------|-------------|-------------|
| **384x384** | 16-20 | ‚úÖ Batch l·ªõn, train nhanh<br>‚úÖ Ti·∫øt ki·ªám VRAM | ‚ùå ƒê·ªô ph√¢n gi·∫£i th·∫•p<br>‚ùå C√≥ th·ªÉ m·∫•t chi ti·∫øt | Khi c·∫ßn train nhanh |
| **512x512** | 8-12 | ‚úÖ C√¢n b·∫±ng t·ªët<br>‚úÖ ƒê·ªô ph√¢n gi·∫£i ƒë·ªß<br>‚úÖ Batch size h·ª£p l√Ω | - | **‚≠ê Khuy·∫øn ngh·ªã ch√≠nh** |
| **640x640** | 4-6 | ‚úÖ ƒê·ªô ph√¢n gi·∫£i cao<br>‚úÖ Chi ti·∫øt t·ªët h∆°n | ‚ùå Batch nh·ªè<br>‚ùå Train ch·∫≠m h∆°n | Khi c·∫ßn ƒë·ªô ch√≠nh x√°c cao |
| **768x768** | 2-4 | ‚úÖ ƒê·ªô ph√¢n gi·∫£i r·∫•t cao | ‚ùå Batch r·∫•t nh·ªè<br>‚ùå C√≥ th·ªÉ OOM | Ch·ªâ khi c·∫ßn thi·∫øt |

**C·∫•u h√¨nh khuy·∫øn ngh·ªã cho EfficientNetB3:**
```python
# Cho EfficientNetB3 tr√™n P100
CROP_SIZE = 512      # Patch size t·ªëi ∆∞u
BATCH_SIZE = 8      # Batch size ph√π h·ª£p v·ªõi 512x512
ARCHITECTURE = "unet_backbone"
BACKBONE_NAME = "EfficientNetB3"
```

**L∆∞u √Ω:**
- EfficientNetB3 l·ªõn h∆°n B0 (~12M params vs ~5M), c·∫ßn nhi·ªÅu VRAM h∆°n
- N·∫øu g·∫∑p OOM (Out of Memory), gi·∫£m batch_size ho·∫∑c gi·∫£m crop_size xu·ªëng 384
- N·∫øu VRAM c√≤n d∆∞, c√≥ th·ªÉ tƒÉng crop_size l√™n 640 ƒë·ªÉ c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng

### Cell 8: Load Data v√† Build Model

```python
# Load labelmap
names, colors = read_labelmap(Path(LABELMAP_PATH))
num_classes = len(names)
print(f"Classes ({num_classes}): {names}")

# Build datasets
train_ds_wrap = EnhancedMultiRootVOCDataset(
    roots=DATA_ROOTS, image_set="train",
    names=names, colors=colors,
    crop_size=CROP_SIZE,
    use_advanced_aug=USE_ADVANCED_AUG
)
val_ds_wrap = EnhancedMultiRootVOCDataset(
    roots=DATA_ROOTS, image_set="val",
    names=names, colors=colors,
    crop_size=CROP_SIZE,
    use_advanced_aug=False
)

print(f"Train samples: {len(train_ds_wrap)}")
print(f"Val samples: {len(val_ds_wrap)}")

# Compute class weights if needed
class_weights = None
if LOSS == "weighted_ce":
    print("Computing class weights...")
    masks = []
    sample_size = min(100, len(train_ds_wrap))
    for i in range(sample_size):
        _, mask = train_ds_wrap.get_item(i)
        masks.append(mask)
    class_weights = compute_class_weights(masks, num_classes, ignore_index=255)
    print(f"Class weights: {class_weights}")

# Create tf.data datasets
train_ds = make_tf_dataset(train_ds_wrap, batch_size=BATCH_SIZE, shuffle=True, ignore_index=255)
val_ds = make_tf_dataset(val_ds_wrap, batch_size=1, shuffle=False, ignore_index=255)
```

### Cell 9: Build Model

```python
# CRITICAL: ƒê·∫£m b·∫£o mixed precision policy v·∫´n l√† float32
# KH√îNG enable mixed_float16 ·ªü ƒë√¢y v√¨ s·∫Ω g√¢y conflict v·ªõi EfficientNet
current_policy = str(mixed_precision.global_policy())
print(f"Current mixed precision policy: {current_policy}")
if 'float32' not in current_policy.lower():
    print("‚ö†Ô∏è  Warning: Policy is not float32! Resetting to float32...")
    mixed_precision.set_global_policy('float32')

# Build model
if ARCHITECTURE == "unet":
    model = build_unet_with_boundary(num_classes=num_classes, dropout=0.2)
elif ARCHITECTURE == "attention_unet":
    model = build_attention_unet(num_classes=num_classes, dropout=0.2)
elif ARCHITECTURE == "unet_plusplus":
    model = build_unet_plusplus(num_classes=num_classes, dropout=0.2, deep_supervision=DEEP_SUPERVISION)
elif ARCHITECTURE == "unet_backbone":
    model = build_unet_with_backbone(
        num_classes=num_classes, 
        backbone="efficientnet", 
        backbone_name=BACKBONE_NAME,  # S·ª≠ d·ª•ng bi·∫øn BACKBONE_NAME t·ª´ Cell 7
        dropout=0.2
    )
else:
    model = build_unet_with_boundary(num_classes=num_classes, dropout=0.2)

print(f"Model parameters: {model.count_params():,}")
model.summary()
```

### Cell 10: Setup Loss v√† Optimizer

```python
# Setup losses
if LOSS == "ce":
    sem_loss = sparse_ce_ignore_index(ignore_index=255, from_logits=True)
elif LOSS == "weighted_ce":
    sem_loss = weighted_sparse_ce_ignore_index(class_weights, ignore_index=255, from_logits=True)
elif LOSS == "focal":
    sem_loss = focal_loss(alpha=0.25, gamma=2.0, ignore_index=255, from_logits=True)
elif LOSS == "tversky":
    sem_loss = tversky_loss(alpha=0.5, beta=0.5, ignore_index=255, from_logits=True)
else:
    sem_loss = sparse_ce_ignore_index(ignore_index=255, from_logits=True)

bce_logits = keras.losses.BinaryCrossentropy(from_logits=True)

# Handle multiple outputs for deep supervision
if ARCHITECTURE == "unet_plusplus" and DEEP_SUPERVISION:
    losses = {
        "ds1": sem_loss,
        "ds2": sem_loss,
        "ds3": sem_loss,
        "sem_logits": sem_loss,
        "boundary_logits": bce_logits
    }
    loss_weights = {
        "ds1": 0.25,
        "ds2": 0.25,
        "ds3": 0.25,
        "sem_logits": 1.0,
        "boundary_logits": 1.0
    }
else:
    losses = {
        "sem_logits": sem_loss,
        "boundary_logits": bce_logits
    }
    loss_weights = {"sem_logits": 1.0, "boundary_logits": 1.0}

optimizer = keras.optimizers.Adam(learning_rate=LR, clipnorm=1.0)
model.compile(optimizer=optimizer, loss=losses, loss_weights=loss_weights)

print("‚úÖ Model compiled!")
```

### Cell 11: Setup Callbacks v√† Train

```python
# T·∫°o th∆∞ m·ª•c l∆∞u model
Path(SAVE_DIR).mkdir(parents=True, exist_ok=True)

# Callbacks
ckpt_path = Path(SAVE_DIR) / f"{ARCHITECTURE}_{LOSS}_best.keras"
eval_cb = EvalCallback(val_ds, num_classes=num_classes, ignore_index=255, ckpt_path=ckpt_path)
# L∆∞u √Ω: EvalCallback t·ª± ƒë·ªông th√™m c√°c metrics v√†o logs:
# - val_loss: negative mIoU (ƒë·ªÉ ReduceLROnPlateau monitor)
# - val_miou: mean Intersection over Union
# - val_pa: Pixel Accuracy
# - val_bce: Binary Cross Entropy (cho boundary)

# Custom callback ƒë·ªÉ l∆∞u checkpoint m·ªói 10 epochs
class PeriodicCheckpoint(keras.callbacks.Callback):
    def __init__(self, filepath, period=10):
        super().__init__()
        self.filepath = filepath
        self.period = period
    
    def on_epoch_end(self, epoch, logs=None):
        # L∆∞u v√†o epoch 10, 20, 30, ... (epoch l√† 0-indexed, n√™n epoch+1)
        if (epoch + 1) % self.period == 0:
            filepath = self.filepath.format(epoch=epoch + 1)
            self.model.save(filepath)
            print(f"Saved checkpoint: {filepath}")

# L∆∞u checkpoint m·ªói 10 epochs
periodic_checkpoint_cb = PeriodicCheckpoint(
    filepath=str(Path(SAVE_DIR) / f"{ARCHITECTURE}_{LOSS}_epoch{{epoch:02d}}.keras"),
    period=10
)

# Gi·∫£m learning rate khi kh√¥ng c·∫£i thi·ªán
# EvalCallback s·∫Ω t·ª± ƒë·ªông th√™m val_loss v√†o logs (t√≠nh t·ª´ negative mIoU)
# Khi val_loss kh√¥ng gi·∫£m trong 5 epochs li√™n ti·∫øp, LR s·∫Ω gi·∫£m 50%
lr_callback = keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',  # EvalCallback t·ª± ƒë·ªông th√™m metric n√†y v√†o logs
    factor=0.5,          # Gi·∫£m LR c√≤n 50% khi kh√¥ng c·∫£i thi·ªán
    patience=5,          # ƒê·ª£i 5 epochs kh√¥ng c·∫£i thi·ªán
    min_lr=1e-6,         # LR t·ªëi thi·ªÉu
    verbose=1             # Hi·ªÉn th·ªã th√¥ng b√°o khi gi·∫£m LR
)

# TensorBoard (optional)
tensorboard_cb = keras.callbacks.TensorBoard(
    log_dir=str(Path(SAVE_DIR) / "logs"),
    histogram_freq=1
)

print("Starting training...")
print(f"Save directory: {SAVE_DIR}")
print("Best model will be saved automatically by EvalCallback")
print("Checkpoints will be saved every 10 epochs")
print("Learning rate will be reduced automatically when val_loss plateaus")

# Train
history = model.fit(
    train_ds,
    epochs=EPOCHS,
    callbacks=[eval_cb, periodic_checkpoint_cb, lr_callback, tensorboard_cb],
    verbose=1
)

print(f"\n‚úÖ Training completed!")
print(f"Best model: {ckpt_path}")
```

---

## üíæ B∆∞·ªõc 7: L∆∞u v√† T·∫£i Model

### L∆∞u model

```python
# Model ƒë√£ ƒë∆∞·ª£c l∆∞u t·ª± ƒë·ªông b·ªüi callbacks v√†o /kaggle/working/models/
# Best model: {SAVE_DIR}/{ARCHITECTURE}_{LOSS}_best.keras
# Checkpoints: {SAVE_DIR}/{ARCHITECTURE}_{LOSS}_epochXX.keras

# File trong /kaggle/working/ s·∫Ω ƒë∆∞·ª£c l∆∞u t·ª± ƒë·ªông khi commit notebook
```

### T·∫£i model ƒë·ªÉ inference

```python
# Load model ƒë√£ train
model_path = f"{SAVE_DIR}/{ARCHITECTURE}_{LOSS}_best.keras"
model = keras.models.load_model(model_path)
print("‚úÖ Model loaded!")
```

### Download Model v·ªÅ m√°y local

```python
# Trong Kaggle notebook, file trong /kaggle/working/ s·∫Ω t·ª± ƒë·ªông ƒë∆∞·ª£c l∆∞u khi commit
# Ho·∫∑c download th·ªß c√¥ng:
from IPython.display import FileLink
FileLink(f"{SAVE_DIR}/{ARCHITECTURE}_{LOSS}_best.keras")
```

---

## üß™ B∆∞·ªõc 7.5: Test Model

### Upload File Test Model

**C√°ch 1: Upload qua File Browser**
- Click v√†o file browser b√™n tr√°i
- Upload file `test_model_kaggle.py` v√†o `/kaggle/working/`

**C√°ch 2: Copy code tr·ª±c ti·∫øp**
```python
# T·∫°o file test_model_kaggle.py
%%writefile /kaggle/working/test_model_kaggle.py
# Paste to√†n b·ªô code t·ª´ file test_model_kaggle.py v√†o ƒë√¢y
```

### Cell Test Model: Ch·∫°y Test v·ªõi Defaults

```python
# Import v√† ch·∫°y test script
import sys
sys.path.insert(0, '/kaggle/working')

from test_model_kaggle import main

# Ch·∫°y v·ªõi defaults (t·ª± ƒë·ªông t√¨m model, image, labelmap)
main()
```

### Cell Test Model: Ch·∫°y Test v·ªõi Arguments

```python
# Import v√† ch·∫°y test script v·ªõi arguments c·ª• th·ªÉ
import sys
sys.path.insert(0, '/kaggle/working')

from test_model_kaggle import main
import sys

# Set arguments
sys.argv = [
    'test_model_kaggle.py',
    '--model_path', '/kaggle/working/models/attention_unet_focal_best.keras',
    '--image_path', '/kaggle/working/data/cheetah/JPEGImages/00000000.jpg',
    '--output_dir', '/kaggle/working/test_results',
    '--labelmap', '/kaggle/working/labelmap.txt',
    '--save_boundary'
]

main()
```

### Ho·∫∑c ch·∫°y tr·ª±c ti·∫øp t·ª´ command line

```python
# Ch·∫°y script nh∆∞ m·ªôt ch∆∞∆°ng tr√¨nh Python
!python /kaggle/working/test_model_kaggle.py \
    --model_path /kaggle/working/models/attention_unet_focal_best.keras \
    --image_path /kaggle/working/data/cheetah/JPEGImages/00000000.jpg \
    --output_dir /kaggle/working/test_results \
    --labelmap /kaggle/working/labelmap.txt \
    --save_boundary
```

### Xem k·∫øt qu·∫£

```python
# Hi·ªÉn th·ªã c√°c file k·∫øt qu·∫£
from pathlib import Path
from IPython.display import Image, display

output_dir = Path("/kaggle/working/test_results")

if output_dir.exists():
    print("üìÅ Files trong test_results:")
    for file in output_dir.glob("*.png"):
        print(f"   - {file.name}")
        
    # Hi·ªÉn th·ªã m·ªôt s·ªë k·∫øt qu·∫£
    if (output_dir / "pred_color.png").exists():
        print("\nüñºÔ∏è  Colorized prediction:")
        display(Image(str(output_dir / "pred_color.png")))
    
    if (output_dir / "pred_overlay.png").exists():
        print("\nüñºÔ∏è  Overlay prediction:")
        display(Image(str(output_dir / "pred_overlay.png")))
else:
    print("‚ùå Th∆∞ m·ª•c test_results ch∆∞a t·ªìn t·∫°i")
    print("üí° H√£y ch·∫°y test script tr∆∞·ªõc")
```

### Download k·∫øt qu·∫£ v·ªÅ m√°y local

```python
# T·∫°o link download cho c√°c file k·∫øt qu·∫£
from IPython.display import FileLink

output_dir = Path("/kaggle/working/test_results")
if output_dir.exists():
    for file in output_dir.glob("*.png"):
        print(f"üì• Download {file.name}:")
        display(FileLink(str(file)))
```

---

## üìä B∆∞·ªõc 8: Monitor Training

### Xem training history

```python
import matplotlib.pyplot as plt

# Plot loss
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='train')
plt.plot(history.history.get('val_loss', []), label='val')
plt.title('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history.get('sem_logits_loss', []), label='semantic')
plt.plot(history.history.get('boundary_logits_loss', []), label='boundary')
plt.title('Component Losses')
plt.legend()

plt.tight_layout()
plt.show()
```

---

## ‚ö†Ô∏è L∆∞u √ù Quan Tr·ªçng

1. **Kaggle Session Limits:**
   - Free tier: ~30 gi·ªù GPU/tu·∫ßn
   - Session timeout: ~9 gi·ªù
   - T·ª± ƒë·ªông l∆∞u khi commit notebook

2. **L∆∞u Model:**
   - ‚ö†Ô∏è **LU√îN l∆∞u model v√†o `/kaggle/working/`** (t·ª± ƒë·ªông l∆∞u khi commit)
   - Model ƒë∆∞·ª£c l∆∞u t·ª± ƒë·ªông b·ªüi callbacks v√†o `SAVE_DIR`
   - File trong `/kaggle/working/` s·∫Ω ƒë∆∞·ª£c l∆∞u khi b·∫°n commit notebook

3. **GPU Limits:**
   - Free tier: ~30 gi·ªù GPU/tu·∫ßn
   - C√≥ th·ªÉ mua Kaggle Pro ƒë·ªÉ c√≥ nhi·ªÅu GPU time h∆°n
   - GPU s·∫Ω t·ª± ƒë·ªông disconnect sau ~9 gi·ªù

4. **Batch Size:**
   - P100 GPU: batch_size = 16-32
   - T4 GPU: batch_size = 16-24
   - T4 x2 GPU: batch_size = 32-48
   - ƒêi·ªÅu ch·ªânh theo VRAM c·ªßa GPU

5. **Data Size:**
   - Dataset c√≥ th·ªÉ l√™n ƒë·∫øn 20GB (free tier) ho·∫∑c 100GB (Pro)
   - Upload dataset m·ªôt l·∫ßn, d√πng l·∫°i nhi·ªÅu l·∫ßn
   - File trong `/kaggle/input/` l√† read-only

6. **Internet Access:**
   - Kaggle notebook c√≥ internet access ƒë·ªÉ download weights t·ª´ ImageNet
   - Kh√¥ng c·∫ßn lo v·ªÅ vi·ªác download pre-trained models

---

## üéØ Quick Start - Ch·∫°y Script T·ª± ƒê·ªông

N·∫øu mu·ªën ch·∫°y nhanh, d√πng script t·ª± ƒë·ªông:

```python
# Ch·∫°y script t·ª± ƒë·ªông (t·ª± ƒë·ªông detect Kaggle v√† set paths)
exec(open('/kaggle/working/model_train_v3_kaggle.py').read())

# Ho·∫∑c g·ªçi h√†m main
from model_train_v3_kaggle import main_unet
main_unet()
```

---

## üìû Troubleshooting

### L·ªói: "No GPU available"
- Settings ‚Üí Accelerator ‚Üí GPU ‚Üí Save
- ƒê·∫£m b·∫£o notebook ƒëang ·ªü ch·∫ø ƒë·ªô GPU (kh√¥ng ph·∫£i CPU)

### L·ªói: "Out of memory"
- Gi·∫£m `BATCH_SIZE` xu·ªëng 8-12
- Gi·∫£m `CROP_SIZE` xu·ªëng 256
- S·ª≠ d·ª•ng gradient checkpointing

### L·ªói: "Dataset not found"
- Ki·ªÉm tra ƒë√£ add dataset v√†o notebook ch∆∞a
- Ki·ªÉm tra t√™n dataset trong code c√≥ ƒë√∫ng kh√¥ng
- Dataset path: `/kaggle/input/YOUR_DATASET_NAME/`

### L·ªói: "Data folder not found sau khi gi·∫£i n√©n"
- Ki·ªÉm tra c·∫•u tr√∫c file ZIP:
  - File ZIP n√™n ch·ª©a folder `data/` ·ªü root
  - Ho·∫∑c khi gi·∫£i n√©n s·∫Ω t·∫°o folder `data/`
- Ki·ªÉm tra Cell 3 (gi·∫£i n√©n) ƒë√£ ch·∫°y th√†nh c√¥ng ch∆∞a
- Xem log gi·∫£i n√©n ƒë·ªÉ bi·∫øt file ƒë∆∞·ª£c gi·∫£i n√©n v√†o ƒë√¢u

### Model kh√¥ng ƒë∆∞·ª£c l∆∞u
- Ki·ªÉm tra `SAVE_DIR` c√≥ t·ªìn t·∫°i kh√¥ng
- ƒê·∫£m b·∫£o ƒëang l∆∞u v√†o `/kaggle/working/`
- File s·∫Ω ƒë∆∞·ª£c l∆∞u khi commit notebook

### Kaggle session timeout
- Kaggle t·ª± ƒë·ªông l∆∞u file trong `/kaggle/working/` khi commit
- Model s·∫Ω ƒë∆∞·ª£c l∆∞u t·ª± ƒë·ªông b·ªüi callbacks
- C√≥ th·ªÉ resume training b·∫±ng c√°ch load checkpoint

---

## ‚úÖ Checklist Tr∆∞·ªõc Khi Train

- [ ] ƒê√£ zip folder `data/` th√†nh file ZIP
- [ ] ƒê√£ t·∫°o dataset tr√™n Kaggle v√† upload file ZIP + `labelmap.txt`
- [ ] ƒê√£ t·∫°o notebook m·ªõi
- [ ] ƒê√£ ch·ªçn GPU trong Settings ‚Üí Accelerator
- [ ] ƒê√£ add dataset v√†o notebook
- [ ] ƒê√£ upload code (`model_train_v3_kaggle.py`) v√† `labelmap.txt` (n·∫øu ch∆∞a c√≥ trong dataset)
- [ ] ƒê√£ ch·∫°y Cell 3 ƒë·ªÉ gi·∫£i n√©n file ZIP (n·∫øu dataset l√† file ZIP)
- [ ] ƒê√£ ki·ªÉm tra data paths trong Cell 5
- [ ] ƒê√£ c√†i ƒë·∫∑t dependencies
- [ ] ƒê√£ c·∫•u h√¨nh `SAVE_DIR` ƒë·ªÉ l∆∞u v√†o `/kaggle/working/`
- [ ] ƒê√£ ki·ªÉm tra batch size ph√π h·ª£p v·ªõi GPU

---

## üîÑ So S√°nh Kaggle vs Colab

| T√≠nh nƒÉng | Kaggle | Colab |
|-----------|--------|-------|
| GPU Time | ~30h/tu·∫ßn (free) | ~12h/ng√†y (free) |
| Session Timeout | ~9 gi·ªù | ~90 ph√∫t idle |
| Data Storage | Dataset (20GB free) | Google Drive (15GB free) |
| Auto Save | T·ª± ƒë·ªông khi commit | Ph·∫£i mount Drive |
| Internet Access | ‚úÖ C√≥ | ‚úÖ C√≥ |
| GPU Types | P100, T4, T4 x2 | T4, A100 |

---

**Ch√∫c b·∫°n train th√†nh c√¥ng tr√™n Kaggle! üöÄ**

