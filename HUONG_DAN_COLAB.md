# üìö H∆∞·ªõng D·∫´n Chi Ti·∫øt Train Model tr√™n Google Colab GPU

## üéØ T·ªïng Quan

H∆∞·ªõng d·∫´n n√†y s·∫Ω gi√∫p b·∫°n train model segmentation tr√™n Google Colab v·ªõi GPU (T4 ho·∫∑c A100) m·ªôt c√°ch chi ti·∫øt t·ª´ng b∆∞·ªõc.

---

## üìã B∆∞·ªõc 1: Upload Code L√™n Colab

### ‚úÖ C√°ch 1: Upload Tr·ª±c Ti·∫øp (ƒê∆°n Gi·∫£n Nh·∫•t - Kh√¥ng C·∫ßn Git)

**Tr√™n Colab:**

1. **M·ªü Colab:** https://colab.research.google.com/
2. **T·∫°o notebook m·ªõi**
3. **Upload files:**
   - Click icon **folder** b√™n tr√°i (Files)
   - Click icon **Upload** (ho·∫∑c k√©o th·∫£)
   - Upload c√°c file:
     - `model_train_v3_improved.py`
     - `inference_improved.py`
     - `labelmap.txt`

**Ho·∫∑c copy code tr·ª±c ti·∫øp:**
```python
# Cell 1: T·∫°o file
%%writefile /content/model_train_v3_improved.py
# Paste to√†n b·ªô code t·ª´ file model_train_v3_improved.py v√†o ƒë√¢y
```

### C√°ch 2: Clone t·ª´ GitHub (N·∫øu Mu·ªën D√πng Git)

**Tr√™n m√°y local:**
```bash
git init
git add .
git commit -m "Initial commit"
git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git
git push -u origin main
```

**Tr√™n Colab:**
```python
!git clone https://github.com/YOUR_USERNAME/YOUR_REPO.git
%cd YOUR_REPO
```

---

## üöÄ B∆∞·ªõc 3: T·∫°o Notebook tr√™n Colab

1. **Truy c·∫≠p:** https://colab.research.google.com/
2. **T·∫°o notebook m·ªõi:** File ‚Üí New notebook
3. **Ch·ªçn GPU:**
   - Runtime ‚Üí Change runtime type
   - Hardware accelerator: **GPU** (T4 ho·∫∑c A100)
   - Save

---

## üîß B∆∞·ªõc 4: Setup M√¥i Tr∆∞·ªùng

### Cell 1: Ki·ªÉm tra GPU

```python
# Ki·ªÉm tra GPU c√≥ s·∫µn kh√¥ng
import tensorflow as tf
print("TensorFlow version:", tf.__version__)
print("GPU Available:", tf.config.list_physical_devices('GPU'))

# Ki·ªÉm tra GPU details
if tf.config.list_physical_devices('GPU'):
    gpu = tf.config.list_physical_devices('GPU')[0]
    print(f"GPU Name: {gpu}")
    # Enable memory growth
    tf.config.experimental.set_memory_growth(gpu, True)
else:
    print("‚ö†Ô∏è Kh√¥ng c√≥ GPU! Vui l√≤ng ch·ªçn GPU trong Runtime -> Change runtime type")
```

### Cell 2: Mount Google Drive

```python
from google.colab import drive
drive.mount('/content/drive')

# Sau khi mount, b·∫°n s·∫Ω th·∫•y ƒë∆∞·ªùng d·∫´n:
# /content/drive/MyDrive/SegmentImg/
```

### Cell 3: C√†i ƒë·∫∑t Dependencies

```python
# C√†i ƒë·∫∑t c√°c package c·∫ßn thi·∫øt
!pip install -q tensorflow>=2.13.0
!pip install -q keras>=2.13.0
!pip install -q scikit-image
!pip install -q scipy
!pip install -q opencv-python
!pip install -q pillow

# Optional: CRF post-processing (n·∫øu c·∫ßn)
# !pip install -q pydensecrf

print("‚úÖ ƒê√£ c√†i ƒë·∫∑t dependencies!")
```

### Cell 4: Upload Code (N·∫øu Ch∆∞a Upload)

**N·∫øu b·∫°n ch∆∞a upload code ·ªü b∆∞·ªõc 1:**

**Option A: Upload file tr·ª±c ti·∫øp**
```python
# S·ª≠ d·ª•ng file browser b√™n tr√°i ƒë·ªÉ upload model_train_v3_improved.py
# Ho·∫∑c d√πng code sau:
from google.colab import files
uploaded = files.upload()  # Ch·ªçn file model_train_v3_improved.py v√† labelmap.txt
```

**Option B: Copy code tr·ª±c ti·∫øp**
```python
# T·∫°o file m·ªõi
%%writefile /content/model_train_v3_improved.py
# Paste to√†n b·ªô code t·ª´ file model_train_v3_improved.py v√†o ƒë√¢y
```

**Option C: N·∫øu code ƒë√£ ·ªü tr√™n Drive**
```python
# Copy t·ª´ Drive
!cp /content/drive/MyDrive/SegmentImg/model_train_v3_improved.py /content/
!cp /content/drive/MyDrive/SegmentImg/labelmap.txt /content/
```

---

## üìÅ B∆∞·ªõc 5: C·∫•u H√¨nh ƒê∆∞·ªùng D·∫´n

### Cell 5: Thi·∫øt l·∫≠p paths

```python
import os
from pathlib import Path

# ƒêi·ªÅu ch·ªânh ƒë∆∞·ªùng d·∫´n theo c·∫•u tr√∫c c·ªßa b·∫°n
# Data ·ªü Google Drive:
DATA_ROOT = "/content/drive/MyDrive/SegmentImg/data"
# Labelmap t·ª´ file ƒë√£ upload:
LABELMAP_PATH = "/content/labelmap.txt"
# Ho·∫∑c n·∫øu labelmap ·ªü Drive:
# LABELMAP_PATH = "/content/drive/MyDrive/SegmentImg/labelmap.txt"

# C√°c dataset folders
DATA_ROOTS = [
    f"{DATA_ROOT}/cheetah",
    f"{DATA_ROOT}/lion",
    f"{DATA_ROOT}/wolf",
    f"{DATA_ROOT}/tiger",
    f"{DATA_ROOT}/hyena",
    f"{DATA_ROOT}/fox",
]

# Th∆∞ m·ª•c l∆∞u model (khuy·∫øn ngh·ªã l∆∞u v√†o Drive)
SAVE_DIR = "/content/drive/MyDrive/SegmentImg/models"

# Ki·ªÉm tra paths
print("Checking data paths...")
for root in DATA_ROOTS:
    if Path(root).exists():
        print(f"‚úÖ {root}")
    else:
        print(f"‚ùå {root} - KH√îNG T·ªíN T·∫†I!")

if Path(LABELMAP_PATH).exists():
    print(f"‚úÖ Labelmap: {LABELMAP_PATH}")
else:
    print(f"‚ùå Labelmap kh√¥ng t·ªìn t·∫°i: {LABELMAP_PATH}")
```

---

## üéì B∆∞·ªõc 6: Import Code v√† Train

### Cell 6: Import v√† Setup

```python
import sys

# Th√™m path ƒë·ªÉ import code
sys.path.insert(0, '/content')  # Code ƒë√£ upload v√†o /content

# Import code
from model_train_v3_improved import (
    read_labelmap, EnhancedMultiRootVOCDataset, 
    make_tf_dataset, build_attention_unet, build_unet_with_boundary,
    build_unet_plusplus, build_unet_with_backbone,
    sparse_ce_ignore_index, weighted_sparse_ce_ignore_index,
    focal_loss, tversky_loss, compute_class_weights,
    EvalCallback
)
import tensorflow as tf
import keras
import numpy as np
import random
from pathlib import Path

print("‚úÖ ƒê√£ import code!")
```

### Cell 7: C·∫•u h√¨nh Training

```python
# C·∫•u h√¨nh training
EPOCHS = 50  # TƒÉng s·ªë epochs khi train tr√™n GPU
BATCH_SIZE = 8  # T4: 8-16, A100: 16-32
LR = 1e-3
CROP_SIZE = 512
ARCHITECTURE = "attention_unet"  # 'unet', 'attention_unet', 'unet_plusplus', 'unet_backbone'
LOSS = "focal"  # 'ce', 'weighted_ce', 'focal', 'tversky'
USE_ADVANCED_AUG = True
DEEP_SUPERVISION = False  # Ch·ªâ d√πng v·ªõi unet_plusplus

# Seed
random.seed(42)
np.random.seed(42)
tf.random.set_seed(42)

print(f"Config:")
print(f"  Architecture: {ARCHITECTURE}")
print(f"  Loss: {LOSS}")
print(f"  Epochs: {EPOCHS}")
print(f"  Batch size: {BATCH_SIZE}")
```

### Cell 8: Load Data v√† Build Model

```python
# Load labelmap
names, colors = read_labelmap(Path(LABELMAP_PATH))
num_classes = len(names)
print(f"Classes ({num_classes}): {names}")

# Build datasets
train_ds_wrap = EnhancedMultiRootVOCDataset(
    roots=DATA_ROOTS, image_set="train",
    names=names, colors=colors,
    crop_size=CROP_SIZE,
    use_advanced_aug=USE_ADVANCED_AUG
)
val_ds_wrap = EnhancedMultiRootVOCDataset(
    roots=DATA_ROOTS, image_set="val",
    names=names, colors=colors,
    crop_size=CROP_SIZE,
    use_advanced_aug=False
)

print(f"Train samples: {len(train_ds_wrap)}")
print(f"Val samples: {len(val_ds_wrap)}")

# Compute class weights if needed
class_weights = None
if LOSS == "weighted_ce":
    print("Computing class weights...")
    masks = []
    sample_size = min(100, len(train_ds_wrap))
    for i in range(sample_size):
        _, mask = train_ds_wrap.get_item(i)
        masks.append(mask)
    class_weights = compute_class_weights(masks, num_classes, ignore_index=255)
    print(f"Class weights: {class_weights}")

# Create tf.data datasets
train_ds = make_tf_dataset(train_ds_wrap, batch_size=BATCH_SIZE, shuffle=True, ignore_index=255)
val_ds = make_tf_dataset(val_ds_wrap, batch_size=1, shuffle=False, ignore_index=255)
```

### Cell 9: Build Model

```python
# Build model
if ARCHITECTURE == "unet":
    model = build_unet_with_boundary(num_classes=num_classes, dropout=0.2)
elif ARCHITECTURE == "attention_unet":
    model = build_attention_unet(num_classes=num_classes, dropout=0.2)
elif ARCHITECTURE == "unet_plusplus":
    model = build_unet_plusplus(num_classes=num_classes, dropout=0.2, deep_supervision=DEEP_SUPERVISION)
elif ARCHITECTURE == "unet_backbone":
    model = build_unet_with_backbone(num_classes=num_classes, backbone="efficientnet", 
                                    backbone_name="EfficientNetB0", dropout=0.2)
else:
    model = build_unet_with_boundary(num_classes=num_classes, dropout=0.2)

print(f"Model parameters: {model.count_params():,}")
model.summary()
```

### Cell 10: Setup Loss v√† Optimizer

```python
# Setup losses
if LOSS == "ce":
    sem_loss = sparse_ce_ignore_index(ignore_index=255, from_logits=True)
elif LOSS == "weighted_ce":
    sem_loss = weighted_sparse_ce_ignore_index(class_weights, ignore_index=255, from_logits=True)
elif LOSS == "focal":
    sem_loss = focal_loss(alpha=0.25, gamma=2.0, ignore_index=255, from_logits=True)
elif LOSS == "tversky":
    sem_loss = tversky_loss(alpha=0.5, beta=0.5, ignore_index=255, from_logits=True)
else:
    sem_loss = sparse_ce_ignore_index(ignore_index=255, from_logits=True)

bce_logits = keras.losses.BinaryCrossentropy(from_logits=True)

# Handle multiple outputs for deep supervision
if ARCHITECTURE == "unet_plusplus" and DEEP_SUPERVISION:
    losses = {
        "ds1": sem_loss,
        "ds2": sem_loss,
        "ds3": sem_loss,
        "sem_logits": sem_loss,
        "boundary_logits": bce_logits
    }
    loss_weights = {
        "ds1": 0.25,
        "ds2": 0.25,
        "ds3": 0.25,
        "sem_logits": 1.0,
        "boundary_logits": 1.0
    }
else:
    losses = {
        "sem_logits": sem_loss,
        "boundary_logits": bce_logits
    }
    loss_weights = {"sem_logits": 1.0, "boundary_logits": 1.0}

optimizer = keras.optimizers.Adam(learning_rate=LR, clipnorm=1.0)
model.compile(optimizer=optimizer, loss=losses, loss_weights=loss_weights)

print("‚úÖ Model compiled!")
```

### Cell 11: Setup Callbacks v√† Train

```python
# T·∫°o th∆∞ m·ª•c l∆∞u model
Path(SAVE_DIR).mkdir(parents=True, exist_ok=True)

# Callbacks
ckpt_path = Path(SAVE_DIR) / f"{ARCHITECTURE}_{LOSS}_best.keras"
eval_cb = EvalCallback(val_ds, num_classes=num_classes, ignore_index=255, ckpt_path=ckpt_path)

# L∆∞u checkpoint ƒë·ªãnh k·ª≥
checkpoint_cb = keras.callbacks.ModelCheckpoint(
    filepath=str(Path(SAVE_DIR) / f"{ARCHITECTURE}_{LOSS}_epoch{{epoch:02d}}.keras"),
    save_freq='epoch',
    period=10,  # L∆∞u m·ªói 10 epochs
    verbose=1
)

# Gi·∫£m learning rate khi kh√¥ng c·∫£i thi·ªán
lr_callback = keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=1e-6,
    verbose=1
)

# TensorBoard (optional)
tensorboard_cb = keras.callbacks.TensorBoard(
    log_dir=str(Path(SAVE_DIR) / "logs"),
    histogram_freq=1
)

print("Starting training...")
print(f"Save directory: {SAVE_DIR}")

# Train
history = model.fit(
    train_ds,
    epochs=EPOCHS,
    callbacks=[eval_cb, checkpoint_cb, lr_callback, tensorboard_cb],
    verbose=1
)

print(f"\n‚úÖ Training completed!")
print(f"Best model: {ckpt_path}")
```

---

## üíæ B∆∞·ªõc 6: L∆∞u v√† T·∫£i Model

### L∆∞u model
```python
# Model ƒë√£ ƒë∆∞·ª£c l∆∞u t·ª± ƒë·ªông b·ªüi callbacks
# Best model: {SAVE_DIR}/{ARCHITECTURE}_{LOSS}_best.keras
# Checkpoints: {SAVE_DIR}/{ARCHITECTURE}_{LOSS}_epochXX.keras
```

### T·∫£i model ƒë·ªÉ inference
```python
# Load model ƒë√£ train
model_path = f"{SAVE_DIR}/{ARCHITECTURE}_{LOSS}_best.keras"
model = keras.models.load_model(model_path)
print("‚úÖ Model loaded!")
```

---

## üìä B∆∞·ªõc 7: Monitor Training

### Xem TensorBoard
```python
# Trong Colab, ch·∫°y:
%load_ext tensorboard
%tensorboard --logdir {SAVE_DIR}/logs
```

### Xem training history
```python
import matplotlib.pyplot as plt

# Plot loss
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='train')
plt.plot(history.history.get('val_loss', []), label='val')
plt.title('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history.get('sem_logits_loss', []), label='semantic')
plt.plot(history.history.get('boundary_logits_loss', []), label='boundary')
plt.title('Component Losses')
plt.legend()

plt.tight_layout()
plt.show()
```

---

## ‚ö†Ô∏è L∆∞u √ù Quan Tr·ªçng

1. **Colab Disconnect:**
   - Colab s·∫Ω disconnect sau ~90 ph√∫t kh√¥ng ho·∫°t ƒë·ªông
   - Gi·ªØ browser tab m·ªü v√† th·ªânh tho·∫£ng scroll ƒë·ªÉ tr√°nh disconnect
   - Ho·∫∑c s·ª≠ d·ª•ng extension nh∆∞ "Colab Alive" ƒë·ªÉ t·ª± ƒë·ªông refresh

2. **L∆∞u Model:**
   - ‚ö†Ô∏è **LU√îN l∆∞u model v√†o Google Drive**, kh√¥ng l∆∞u v√†o `/content` (s·∫Ω m·∫•t khi disconnect)
   - Model ƒë∆∞·ª£c l∆∞u t·ª± ƒë·ªông b·ªüi callbacks v√†o `SAVE_DIR`

3. **GPU Limits:**
   - Free tier: ~12 gi·ªù GPU/ng√†y
   - N·∫øu h·∫øt quota, ƒë·ª£i ƒë·∫øn ng√†y h√¥m sau ho·∫∑c upgrade Colab Pro

4. **Batch Size:**
   - T4 GPU: batch_size = 8-16
   - A100 GPU: batch_size = 16-32
   - ƒêi·ªÅu ch·ªânh theo VRAM c·ªßa GPU

5. **Data Size:**
   - N·∫øu dataset l·ªõn, upload l√™n Drive v√† mount
   - Tr√°nh upload tr·ª±c ti·∫øp v√†o Colab (c√≥ th·ªÉ b·ªã gi·ªõi h·∫°n)

---

## üéØ Quick Start Commands

N·∫øu mu·ªën ch·∫°y nhanh, copy to√†n b·ªô code t·ª´ file `colab_train.py` v√†o m·ªôt cell v√† ch·∫°y:

```python
# Ch·∫°y script t·ª± ƒë·ªông
exec(open('/content/colab_train.py').read())
```

Ho·∫∑c s·ª≠ d·ª•ng script ƒë√£ t·∫°o:

```python
# Upload colab_train.py tr∆∞·ªõc, sau ƒë√≥:
import colab_train
```

---

## üìû Troubleshooting

### L·ªói: "No GPU available"
- Runtime ‚Üí Change runtime type ‚Üí GPU ‚Üí Save

### L·ªói: "Out of memory"
- Gi·∫£m `BATCH_SIZE`
- Gi·∫£m `CROP_SIZE`
- S·ª≠ d·ª•ng gradient checkpointing

### L·ªói: "Drive mount failed"
- Ch·∫°y l·∫°i cell mount Drive
- ƒê·∫£m b·∫£o cho ph√©p Colab truy c·∫≠p Drive

### Model kh√¥ng ƒë∆∞·ª£c l∆∞u
- Ki·ªÉm tra `SAVE_DIR` c√≥ t·ªìn t·∫°i kh√¥ng
- ƒê·∫£m b·∫£o ƒë√£ mount Drive n·∫øu l∆∞u v√†o Drive

---

## ‚úÖ Checklist Tr∆∞·ªõc Khi Train

- [ ] ƒê√£ ch·ªçn GPU trong Runtime settings
- [ ] ƒê√£ mount Google Drive (n·∫øu d√πng)
- [ ] ƒê√£ upload code v√† data
- [ ] ƒê√£ ki·ªÉm tra data paths
- [ ] ƒê√£ c√†i ƒë·∫∑t dependencies
- [ ] ƒê√£ c·∫•u h√¨nh `SAVE_DIR` ƒë·ªÉ l∆∞u v√†o Drive
- [ ] ƒê√£ ki·ªÉm tra batch size ph√π h·ª£p v·ªõi GPU

---

**Ch√∫c b·∫°n train th√†nh c√¥ng! üöÄ**

